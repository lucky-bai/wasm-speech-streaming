<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>WASM Streaming Speech Recognition</title>
    <style>
      @import url("https://fonts.googleapis.com/css2?family=Source+Sans+3:wght@300;400;600;700&display=swap");
      html, body { font-family: "Source Sans 3", system-ui, -apple-system, Segoe UI, Roboto, sans-serif; }
    </style>
    <script src="css/tailwind-3.4.17.js"></script>
    <script type="module">
      const MODEL_ID = "moshi_1b_en_fr_q4k";
      const WEIGHTS_URL = "https://huggingface.co/efficient-nlp/stt-1b-en_fr-quantized/resolve/main/model-q4k.gguf";
      const MIMI_URL = "https://huggingface.co/efficient-nlp/stt-1b-en_fr-quantized/resolve/main/mimi-pytorch-e351c8d8@125.safetensors";
      const TOKENIZER_URL = "https://huggingface.co/efficient-nlp/stt-1b-en_fr-quantized/resolve/main/tokenizer_en_fr_audio_8000.json";
      const CONFIG_URL = "https://huggingface.co/efficient-nlp/stt-1b-en_fr-quantized/resolve/main/config.json";

      const moshiWorker = new Worker("./moshiWorker.js", { type: "module" });
      let mediaRecorder = null;
      let audioChunks = [];
      let isRecording = false;
      let audioStream = null;
      let audioContext = null;
      let processor = null;
      let source = null;

      function updateStatusDiv(message) {
        document.querySelector("#status-div").textContent = message;
      }

      function updateStatus(data) {
        const { status, message, word } = data;
        const outputDiv = document.querySelector("#output-generation");
        
        if (status === "loading" || status === "decoding") {
          updateStatusDiv(message || (status === "loading" ? "Loading..." : "Decoding..."));
        } else if (status === "streaming") {
          // Add new word to the transcription in real-time
          if (outputDiv.textContent) {
            outputDiv.textContent += " " + word;
          } else {
            outputDiv.textContent = word;
          }
          outputDiv.hidden = false;
        } else if (status === "complete") {
          updateStatusDiv("Ready");
        }
      }

      async function startMicrophone() {
        try {
          audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });
          updateStatusDiv("Microphone access granted");
          
          audioContext = new AudioContext({ sampleRate: 24000 });
          source = audioContext.createMediaStreamSource(audioStream);
          
          processor = audioContext.createScriptProcessor(1024, 1, 1);
          
          processor.onaudioprocess = function(event) {
            if (!isRecording) return;
            
            const inputBuffer = event.inputBuffer;
            const inputData = inputBuffer.getChannelData(0);
            
            // Log each frame of audio data
            console.log('Audio frame:', inputData.length, 'samples');
            console.log('Sample values:', Array.from(inputData.slice(0, 10)));
          };
          
          source.connect(processor);
          processor.connect(audioContext.destination);
          
        } catch (error) {
          updateStatusDiv("Microphone access denied: " + error.message);
          throw error;
        }
      }
      
      function stopMicrophone() {
        // Disconnect audio nodes
        if (processor) {
          processor.disconnect();
          processor = null;
        }
        if (source) {
          source.disconnect();
          source = null;
        }
        if (audioContext) {
          audioContext.close();
          audioContext = null;
        }
        
        // Stop media stream
        if (audioStream) {
          audioStream.getTracks().forEach(track => track.stop());
          audioStream = null;
        }
        
        updateStatusDiv("Microphone stopped");
      }


      document.querySelector("#speech-button").addEventListener("click", async () => {
        const button = document.querySelector("#speech-button");
        
        if (!isRecording) {
          try {
            updateStatusDiv("Requesting microphone access...");
            await startMicrophone();
            
            isRecording = true;
            button.textContent = "Stop Speech";
            button.className = "bg-red-600 hover:bg-red-700 text-white font-normal py-2 px-4 rounded";
            updateStatusDiv("Listening...");
            
            // Clear previous transcription
            document.querySelector("#output-generation").textContent = "";
            document.querySelector("#output-generation").hidden = true;
            
          } catch (error) {
            console.error('Error starting microphone:', error);
            updateStatusDiv("Error: " + error.message);
          }
        } else {
          stopMicrophone();
          
          isRecording = false;
          button.textContent = "Start Speech";
          button.className = "bg-gray-700 hover:bg-gray-800 text-white font-normal py-2 px-4 rounded";
          updateStatusDiv("Stopped");
        }
      });
    </script>
  </head>
  <body class="container max-w-4xl mx-auto p-4">
    <main class="grid grid-cols-1 gap-8 relative">
      <div>
        <h1 class="text-4xl font-bold">WASM Streaming Speech Recognition</h1>
        <p class="text-gray-700">
          Transcribe audio from your microphone in real time in the browser using Rust/WASM.
          This demo runs entirely offline on your CPU after downloading a ~950 MB model.
          It understands English and French, and uses the
          <a href="https://huggingface.co/kyutai/stt-1b-en_fr" target="_blank" class="underline hover:text-blue-600">Kyutai STT model</a>
          together with a WASM runtime built in 
          <a href="https://github.com/huggingface/candle/" target="_blank" class="underline hover:text-blue-600">Candle</a>.
        </p>
      </div>

      <div>
        <button id="speech-button" class="bg-gray-700 hover:bg-gray-800 text-white font-normal py-2 px-4 rounded">
          Start Speech
        </button>
        <div class="mt-2 text-gray-600">
          Status: <span id="status-div">Ready to start</span>
        </div>
      </div>

      <div>
        <h3 class="font-medium">Transcription:</h3>
        <div class="min-h-[200px] bg-slate-100 text-gray-700 p-4 rounded-md">
          <p id="output-generation" hidden></p>
          <span id="output-placeholder" class="font-light text-gray-500">Click "Start Speech" to begin transcription</span>
        </div>
      </div>

      <div class="mt-4 p-3 bg-gray-50 text-gray-700 rounded-md">
        ðŸ’¡ This demo shows offline transcription in your browser.  
        For more accurate cloud transcription and real-time LLM grammar correction, check out  
        <a href="https://voicewriter.io" target="_blank" class="underline hover:text-blue-600">Voice Writer</a>.
      </div>
    </main>
  </body>
</html>